---
title: "AML"
format: html
editor: visual
---

```{r}
if (!requireNamespace("BiocManager", quietly = TRUE))
    install.packages("BiocManager")
if (!requireNamespace("doParallel", quietly = TRUE))
    install.packages("doParallel")
library(tidyverse)
library(readr)
library(biomaRt)
library(data.table)
library(doParallel)
library(future)
library(furrr)
```

## Metadata pre-processing

```{r}
metadata <- read_tsv("TARGET_AML_Clinical.tsv")
```

```{r}
# Drop last three rows since they don't contain proper information
#metadata <- head(metadata, n = nrow(metadata) - 3)

# Identify columns where all values are NA or empty strings
na_cols <- sapply(metadata, function(x) all(is.na(x)))
empty_cols <- sapply(metadata, function(x) all(x == ""))

# Combine NA and empty columns and identify which to drop
cols_to_drop <- which(na_cols | empty_cols)

# Drop the identified columns
metadata <- metadata[, -cols_to_drop]

# Keep only the variables we talked about
metadata <- metadata[, c("TARGET USI", "Vital Status", "Overall Survival Time in Days", "MRD at end of course 1", "CR status at end of course 1")]

# Drop rows with NAs
metadata <- na.omit(metadata)

# Keep only rows with the values we need
metadata <- metadata[metadata$"MRD at end of course 1" %in% c("Yes", "No"), ]
metadata <- metadata[metadata$"CR status at end of course 1" %in% c("CR", "not in CR"), ]

```

### We care about the following variables:

-   Vital status (alive/dead)

-   Overall survival time in days

-   MRD (for stratifying patients)

-   CR status (for stratifying patients)

Although I kept the others in, maybe we find something interesting.

## Counts data pre-processing

Since we have different tsv files for every sample, we need to load them all, pre-process them and merge them.

We can create two files:

1.  One with gene names as row names and samples as column names. Here, because multiple gene ids may map to the same gene name we must either average those (approach A), keep the most expressed isoform (approach B) or another handling.

2.  The same but with gene ids as row names, samples as column names plus another column with the gene names. Could be useful if we try CAR-T cell target evaluation for isoforms (maybe this is too much).

```{r}
# First file: doesn't include gene_id
# Sum different gene_id (approach B)

# Step 1: List all TSV files in the counts directory
counts_files <- list.files(path = "counts/", pattern = "\\.tsv$", full.names = TRUE)

# Step 2: Set up parallel plan
plan(multisession, workers = parallel::detectCores() - 1)

# Step 3: Define the processing function for each file
process_file <- function(file) {
  # Load the file into a dataframe
  df <- read.delim(file, sep = "\t", header = TRUE)

  # Convert to tibble to work smoothly with dplyr functions
  df <- as_tibble(df)

  # Perform preprocessing
  # Drop columns (assuming we keep stranded_first)
  columns_to_drop <- c("gene_id", "gene_type", "unstranded", "stranded_second", "tpm_unstranded", "fpkm_unstranded", "fpkm_uq_unstranded")
  df <- dplyr::select(df, -all_of(columns_to_drop))

  # Drop columns that have all NA values
  df <- dplyr::select(df, where(~ any(!is.na(.))))

  # Rename the selected column to the file name
  new_column_name <- gsub(".*/|\\.tsv$", "", file)
  df <- dplyr::rename(df, !!new_column_name := stranded_first)

  # Group by gene_name and take the sum for duplicated gene names
  df <- df |>
    group_by(gene_name) |>
    summarise(across(everything(), sum, na.rm = TRUE), .groups = "drop")

  # Round all values (except the "gene_name" column) to the nearest integer
  df <- df |>
    mutate(across(where(is.numeric), round))

  return(df)
}

# Step 4: Apply the function to each file in parallel using furrr::future_map
counts_dataframe_list <- furrr::future_map(counts_files, process_file)

# Step 5: Merge all dataframes in the list by "gene_name"
merged_counts <- reduce(counts_dataframe_list, full_join, by = "gene_name")

# Step 6: Make gene_names the row names
merged_counts <- merged_counts |> column_to_rownames(var = "gene_name")

# Check the merged dataframe
print(merged_counts)

```

```{r}
# First file: doesn't include gene_id 
# Keep the most expressed isoform (approach B)

# Step 1: List all TSV files in the counts directory
counts_files <- list.files(path = "counts/", pattern = "\\.tsv$", full.names = TRUE)

# Step 2: Set up parallel plan
plan(multisession, workers = parallel::detectCores() - 1)

# Step 3: Define the processing function for each file
process_file <- function(file) {
  # Load the file into a dataframe
  df <- read.delim(file, sep = "\t", header = TRUE)

  # Convert to tibble to work smoothly with dplyr functions
  df <- as_tibble(df)

  # Perform preprocessing
  # Drop columns (assuming we keep stranded_first)
  columns_to_drop <- c("gene_id", "gene_type", "unstranded", "stranded_second", "tpm_unstranded", "fpkm_unstranded", "fpkm_uq_unstranded")
  df <- dplyr::select(df, -all_of(columns_to_drop))

  # Drop columns that have all NA values
  df <- dplyr::select(df, where(~ any(!is.na(.))))

  # Rename the selected column to the file name
  new_column_name <- gsub(".*/|\\.tsv$", "", file)
  df <- dplyr::rename(df, !!new_column_name := stranded_first)

  # Group by gene_name and take the maximum value for duplicated gene names (most expressed isoform)
  df <- df |>
    group_by(gene_name) |>
    summarise(across(everything(), max, na.rm = TRUE), .groups = "drop")

  # Round all values (except the "gene_name" column) to the nearest integer
  df <- df |>
    mutate(across(where(is.numeric), round))

  return(df)
}

# Step 4: Apply the function to each file in parallel using furrr::future_map
counts_dataframe_list <- furrr::future_map(counts_files, process_file)

# Step 5: Merge all dataframes in the list by "gene_name"
merged_counts_max <- reduce(counts_dataframe_list, full_join, by = "gene_name")

# Step 6: Make gene_names the row names
merged_counts_max <- merged_counts_max |> column_to_rownames(var = "gene_name")

# Check the merged dataframe
print(merged_counts_max)

```

```{r}
# Second file: includes gene_id

# Step 1: Initialize an empty list to store dataframes
counts_dataframe_list_with_ids <- list()

# Step 2: Loop through each file, read it, process, and store in the list
for (file in counts_files) {
  # Load the file into a dataframe
  df <- read.delim(file, sep = "\t", header = TRUE)

  # Perform preprocessing
  # Drop columns (assuming we keep stranded_first and gene_id)
  columns_to_drop <- c("gene_type", "unstranded", "stranded_second", "tpm_unstranded", "fpkm_unstranded", "fpkm_uq_unstranded")
  df <- df[, !(names(df) %in% columns_to_drop)]
  
  # Drop columns that have all NA values
  df <- df[, colSums(!is.na(df)) > 0]
  
  # Rename columns (assuming we keep stranded_first)
  new_column_name <- gsub(".*/|\\.tsv$", "", file)
  colnames(df)[colnames(df) == "stranded_first"] <- new_column_name
  
  # Store the dataframe in the list
  counts_dataframe_list_with_ids[[file]] <- df
}

# Step 3: Merge all dataframes in the list by both "gene_name" and "gene_id"
merged_counts_with_ids <- reduce(counts_dataframe_list_with_ids, full_join, by = c("gene_name", "gene_id"))

# Check the merged dataframe
print(merged_counts_with_ids)
```
